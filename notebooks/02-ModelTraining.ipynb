{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DecisionforestClassifier' from 'sklearn.tree' (C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\tree\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Classes do modelo de aprendizado\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionforestClassifier\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DecisionforestClassifier' from 'sklearn.tree' (C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\tree\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy.stats as ss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Classes do modelo de aprendizado\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Funções de avaliação dos modelos\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Gráficos\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes e Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PALETTE = 'RdYlGn'\n",
    "PALETTE = 'viridis'\n",
    "SEED = 42\n",
    "N_ITER = 10\n",
    "TRAINVAL_SPLITS = 5  # Ver se tudo bem!\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colors(num_colors):\n",
    "    colors = px.colors.sample_colorscale(PALETTE, [n/(num_colors - 1) for n in range(num_colors)])\n",
    "    \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X, y, param_grid, n_iter=10, n_trainval_splits=10):\n",
    "    skf = StratifiedKFold(n_splits=n_iter, shuffle=True)   # Shuffle?\n",
    "    skf_folds = skf.split(X, y)\n",
    "    \n",
    "    runs_metrics = {}\n",
    "    for n, (trainval_idx, test_idx) in enumerate(tqdm(skf_folds, total=n_iter)):\n",
    "        X_trainval = X[trainval_idx]\n",
    "        y_trainval = y[trainval_idx]\n",
    "        \n",
    "        X_test = X[test_idx]\n",
    "        y_test = y[test_idx]\n",
    "\n",
    "        best_params = get_best_params(model, X_trainval, y_trainval, param_grid, n_trainval_splits=n_trainval_splits, \n",
    "                                      display_results=False)\n",
    "        \n",
    "        model.set_params(**best_params)\n",
    "        model.fit(X_trainval, y_trainval)\n",
    "        \n",
    "        model_metrics = evaluate_model_performance(model, X_test, y_test)\n",
    "        model_metrics['best_params'] = best_params\n",
    "        runs_metrics[n] = model_metrics\n",
    "\n",
    "    runs_metrics = aggregate_run_metrics(runs_metrics)\n",
    "    return runs_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(model, X_trainval, y_trainval, param_grid, n_trainval_splits=10, display_results=False):\n",
    "    skf = StratifiedKFold(n_splits=n_trainval_splits, shuffle=True)  # Shuffle?\n",
    "    \n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, refit=False, cv=skf, n_jobs=-1)\n",
    "    grid_search.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    if display_results:\n",
    "        df_res = pd.DataFrame(grid_search.cv_results_)\n",
    "        df_res = df_res.sort_values('rank_test_score', ascending=True)\n",
    "        display(df_res)\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    report_dict = classification_report(y, y_pred, output_dict=True)\n",
    "    report_dict['cm'] = cm\n",
    "\n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_run_metrics(runs_res):\n",
    "    runs_ids = runs_res.keys()\n",
    "    labels = [label for label in runs_res[0].keys() if label not in ['accuracy', 'micro avg', 'best_params', 'macro avg', \n",
    "                                                                     'weighted avg', 'cm']]  # Porco?\n",
    "\n",
    "    accuracies = [runs_res[i]['accuracy'] for i in runs_ids]\n",
    "    cms = [runs_res[i]['cm'] for i in runs_ids]\n",
    "    best_params = [runs_res[i]['best_params'] for i in runs_ids]\n",
    "    recalls = {i: [runs_res[j][i]['recall'] for j in runs_ids] for i in labels}\n",
    "    precisions = {i: [runs_res[j][i]['precision'] for j in runs_ids] for i in labels}\n",
    "    f1_scores = {i: [runs_res[j][i]['f1-score'] for j in runs_ids] for i in labels}\n",
    "\n",
    "    metrics = {\n",
    "        'accuracies': accuracies,\n",
    "        'cms': cms,\n",
    "        'f1-scores': f1_scores,\n",
    "        'recalls': recalls,\n",
    "        'precisions': precisions,\n",
    "        'best_params': best_params\n",
    "      }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res(res):  # Cumulo da porquice, mas funciona bem\n",
    "    print(f\"===> ACURÁCIA MÉDIA <===\\n{np.mean(res['accuracies']):.4f}\", end='\\n\\n')\n",
    "    print(f\"===> MATRIZ DE CONFUSÃO GERAL <===\\n{np.sum(res['cms'], axis=0)}\", end='\\n\\n')\n",
    "    \n",
    "    print('===> RECALL, PRECISION E F1-SCORE MÉDIO <===') \n",
    "    print(f\"{'Label'.ljust(10)} | {'Recall'.ljust(10)} | {'Precision'.ljust(10)} | {'F1-Score'.ljust(10)}\")\n",
    "    print('-'*48)\n",
    "    for l in res['recalls'].keys():  # Igual para todos\n",
    "        mean_recall = np.mean(res['recalls'][l])\n",
    "        mean_precision = np.mean(res['precisions'][l])\n",
    "        mean_f1_score = np.mean(res['f1-scores'][l])\n",
    "        print(f'{l.ljust(10)} | {str(np.round(mean_recall, 4)).ljust(10)} | {str(np.round(mean_precision, 4)).ljust(10)} | '\n",
    "              f'{str(np.round(mean_f1_score, 4)).ljust(10)}')\n",
    "\n",
    "    print('\\n===> MELHORES HIPERPARÂMETROS <===') \n",
    "    print(f\"{'Ocorrências'.ljust(12)} | {'Valores'.ljust(75)}\")\n",
    "    print('-'*130)\n",
    "    params_counts = Counter(tuple(param.items()) for param in res['best_params'])\n",
    "    params_counts_mc = params_counts.most_common()\n",
    "    \n",
    "    for pcm in params_counts_mc:  # [:5] para mostrar apenas top 5\n",
    "        pcm_values = pcm[0]\n",
    "        pcm_occ = pcm[1]\n",
    "    \n",
    "        print(f'{str(pcm_occ).ljust(12)} | {pcm_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=accuracies, nbinsx=5))\n",
    "    fig.add_vline(x=np.mean(accuracies), line_dash='dash', annotation_text=f'Acurácia Média: {np.mean(accuracies):.2f}')\n",
    "    fig.update_layout(title='Histograma da Acurácia', height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_metrics(label_score, score_title):\n",
    "    labels = list(label_score.keys())\n",
    "    marker_colors = generate_colors(len(labels))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, l in enumerate(labels):\n",
    "        name = f'Label {l}'\n",
    "        color = marker_colors[i]\n",
    "        f1_score = label_score[l]\n",
    "\n",
    "        fig.add_trace(go.Box(y=f1_score, name=name, marker_color=color, legendgroup=i))\n",
    "\n",
    "    fig.update_layout(title=f'Boxplots dos {score_title.title()} por label', height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cms):\n",
    "    cm = np.sum(cms, axis=0)\n",
    "    cm_mean = np.mean(cms, axis=0)\n",
    "    cm_recall = cm/np.sum(cm, axis=1)\n",
    "    cm_precision = cm/np.sum(cm, axis=0)\n",
    "    # cm_f1score = np.nan_to_num(2*(cm_precision*cm_recall)/(cm_precision + cm_recall))  # Faz sentido?\n",
    "    \n",
    "    axis_labels = list(range(len(cm)))\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=2, shared_xaxes=True, shared_yaxes=True, vertical_spacing=0.1,\n",
    "                        subplot_titles=['Padrão', 'Média', 'Recall', 'Precision'])\n",
    "    \n",
    "    fig.add_trace(go.Heatmap(x=axis_labels, y=axis_labels, z=cm, text=cm, texttemplate='%{text}', \n",
    "                             showscale=False, colorscale=PALETTE), \n",
    "                  row=1, col=1)\n",
    "\n",
    "    # Média\n",
    "    fig.add_trace(go.Heatmap(x=axis_labels, y=axis_labels, z=cm_mean, text=cm_mean, texttemplate='%{text:.2f}', \n",
    "                             showscale=False, colorscale=PALETTE), \n",
    "                  row=1, col=2)\n",
    "    \n",
    "    # F1-score\n",
    "    # fig.add_trace(go.Heatmap(x=axis_labels, y=axis_labels, z=cm_f1score, text=cm_f1score, texttemplate='%{text:.2f}', \n",
    "    #                          showscale=False, colorscale=PALETTE), \n",
    "    #               row=1, col=2)\n",
    "\n",
    "    fig.add_trace(go.Heatmap(x=axis_labels, y=axis_labels, z=cm_recall, text=cm_recall, texttemplate='%{text:.2f}', \n",
    "                             showscale=False, colorscale=PALETTE), \n",
    "                  row=2, col=1)\n",
    "    fig.add_trace(go.Heatmap(x=axis_labels, y=axis_labels, z=cm_precision, text=cm_precision, texttemplate='%{text:.2f}', \n",
    "                             showscale=False, colorscale=PALETTE), \n",
    "                  row=2, col=2)\n",
    "\n",
    "    fig.update_layout(title='Matrizes de Confusão', yaxis1_title='Real', yaxis3_title='Real', xaxis3_title='Predito', \n",
    "                      xaxis4_title='Predito', yaxis1_autorange='reversed', yaxis2_autorange='reversed', \n",
    "                      yaxis3_autorange='reversed', height=800)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/processed/steel-plates-fault.pkl')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target']).values\n",
    "y = df['target'].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'model__n_neighbors': [5, 1, 3, 10],\n",
    "    'model__p': [2, 1],\n",
    "    'model__weights': ['uniform', 'distance']    \n",
    "}\n",
    "\n",
    "knn_model = Pipeline([('scaler', StandardScaler()) , ('model', KNeighborsClassifier())])\n",
    "knn_res = validate_model(knn_model, X, y, knn_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(knn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_acc = knn_res['accuracies']\n",
    "\n",
    "plot_accuracies(knn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_f1 = knn_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(knn_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cms = knn_res['cms']\n",
    "\n",
    "plot_confusion_matrix(knn_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20, 100],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_res = validate_model(tree_model, X, y, tree_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(tree_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_acc = tree_res['accuracies']\n",
    "\n",
    "plot_accuracies(tree_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_f1 = tree_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(tree_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cms = tree_res['cms']\n",
    "\n",
    "plot_confusion_matrix(tree_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floresta Aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20, 100],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    #'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [100, 50, 200, 500]\n",
    "}\n",
    "\n",
    "forest_model = RandomForestClassifier()\n",
    "forest_res = validate_model(forest_model, X, y, forest_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(forest_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_acc = forest_res['accuracies']\n",
    "\n",
    "plot_accuracies(forest_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_f1 = forest_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(forest_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cms = forest_res['cms']\n",
    "\n",
    "plot_confusion_matrix(forest_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    'model__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "    'model__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "    'model__C': [1, 0.1, 0.01, 10, 100]\n",
    "}\n",
    "\n",
    "lr_model = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(max_iter=5000))])\n",
    "lr_res = validate_model(lr_model, X, y, lr_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(lr_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc = lr_res['accuracies']\n",
    "\n",
    "plot_accuracies(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_f1 = lr_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(lr_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cms = lr_res['cms']\n",
    "\n",
    "plot_confusion_matrix(lr_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test = ss.normaltest(X[:,1])\n",
    "p_values = norm_test.pvalue\n",
    "p_values >= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_grid = {\n",
    "    'model__C': [1, 0.1, 0.01, 10, 100],\n",
    "    'model__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'model__gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "svc_model = Pipeline([('scaler', StandardScaler()), ('model', SVC())])\n",
    "svc_res = validate_model(svc_model, X, y, svc_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(svc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_acc = svc_res['accuracies']\n",
    "\n",
    "plot_accuracies(svc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_f1 = svc_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(svc_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_cms = svc_res['cms']\n",
    "\n",
    "plot_confusion_matrix(svc_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_param_grid = {\n",
    "    'model__hidden_layer_sizes': [(10, ), (50,), (10, 10), (10, 30, 10)],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__solver': ['adam', 'sgd'], # Justificar ausência do lfbgf\n",
    "    'model__alpha': [0.0001, 0.001, 0.01],\n",
    "    'model__learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "    #'model__batch_size': ['auto', 4, 8, 16, 32]\n",
    "}\n",
    "\n",
    "mlp_model = Pipeline([('scaler', StandardScaler()), ('model', MLPClassifier(max_iter=1000))])\n",
    "mlp_res = validate_model(mlp_model, X, y, mlp_param_grid, n_iter=N_ITER, n_trainval_splits=TRAINVAL_SPLITS)\n",
    "print_res(mlp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_acc = mlp_res['accuracies']\n",
    "\n",
    "plot_accuracies(mlp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_f1 = mlp_res['f1-scores']\n",
    "\n",
    "plot_label_metrics(mlp_f1, 'f1-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cms = mlp_res['cms']\n",
    "\n",
    "plot_confusion_matrix(mlp_cms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
